<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/lib/animate-css/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"8.0.0-rc.4","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}}};
  </script>

  <meta name="description" content="全文共3487字，是暑期学习TensorFlow的记录，推荐按照顺序进行阅读。">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow Learning Note">
<meta property="og:url" content="http://yoursite.com/2021/06/23/TensorFlow%20Learning%20Note/index.html">
<meta property="og:site_name" content="Silence">
<meta property="og:description" content="全文共3487字，是暑期学习TensorFlow的记录，推荐按照顺序进行阅读。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-06-23T12:46:18.752Z">
<meta property="article:modified_time" content="2020-09-06T08:04:47.977Z">
<meta property="article:author" content="Silence">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2021/06/23/TensorFlow%20Learning%20Note/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>TensorFlow Learning Note | Silence</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Silence" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Silence</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-lover">

    <a href="/lover/" rel="section"><i class="fa fa-heart fa-fw"></i>lover</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
  </ul>
</nav>




</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#张量、计算图、会话"><span class="nav-number">1.</span> <span class="nav-text">张量、计算图、会话</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概述"><span class="nav-number">1.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量-tensor"><span class="nav-number">1.2.</span> <span class="nav-text">张量(tensor)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算图"><span class="nav-number">1.3.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#会话-Session"><span class="nav-number">1.4.</span> <span class="nav-text">会话(Session)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数"><span class="nav-number">1.5.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络搭建"><span class="nav-number">1.6.</span> <span class="nav-text">神经网络搭建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播"><span class="nav-number">2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">3.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络八股"><span class="nav-number">3.1.</span> <span class="nav-text">神经网络八股</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-number">4.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数"><span class="nav-number">4.1.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NN复杂度"><span class="nav-number">4.2.</span> <span class="nav-text">NN复杂度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义损失函数"><span class="nav-number">4.3.</span> <span class="nav-text">自定义损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交叉熵"><span class="nav-number">4.4.</span> <span class="nav-text">交叉熵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率"><span class="nav-number">5.</span> <span class="nav-text">学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#指数衰减学习率"><span class="nav-number">5.1.</span> <span class="nav-text">指数衰减学习率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#滑动平均（影子值）"><span class="nav-number">6.</span> <span class="nav-text">滑动平均（影子值）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-number">7.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建八股"><span class="nav-number">8.</span> <span class="nav-text">搭建八股</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MNIST数据集"><span class="nav-number">9.</span> <span class="nav-text">MNIST数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本操作"><span class="nav-number">9.1.</span> <span class="nav-text">基本操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">10.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概述-1"><span class="nav-number">10.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积计算"><span class="nav-number">10.2.</span> <span class="nav-text">卷积计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#池化-Pooling"><span class="nav-number">10.3.</span> <span class="nav-text">池化(Pooling)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#舍弃-Dropout"><span class="nav-number">10.4.</span> <span class="nav-text">舍弃(Dropout)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结构"><span class="nav-number">10.5.</span> <span class="nav-text">结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">11.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Silence"
      src="https://i.loli.net/2020/07/05/Ml7X4UshoCJgn52.jpg">
  <p class="site-author-name" itemprop="name">Silence</p>
  <div class="site-description" itemprop="description">I'm right here waiting for you.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/SilenceX12138" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SilenceX12138" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/silencejiang12138" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;silencejiang12138" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/06/23/TensorFlow%20Learning%20Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/07/05/Ml7X4UshoCJgn52.jpg">
      <meta itemprop="name" content="Silence">
      <meta itemprop="description" content="I'm right here waiting for you.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Silence">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TensorFlow Learning Note
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-23 20:46:18" itemprop="dateCreated datePublished" datetime="2021-06-23T20:46:18+08:00">2021-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-06 16:04:47" itemprop="dateModified" datetime="2020-09-06T16:04:47+08:00">2020-09-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/06/23/TensorFlow%20Learning%20Note/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/23/TensorFlow%20Learning%20Note/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>全文共<code>3487</code>字，是暑期学习TensorFlow的记录，推荐按照顺序进行阅读。</p>
<a id="more"></a>
<h2 id="张量、计算图、会话"><a href="#张量、计算图、会话" class="headerlink" title="张量、计算图、会话"></a>张量、计算图、会话</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li>用<strong>张量</strong>表示数据，用<strong>计算图</strong>搭建神经网络，用<strong>会话</strong>执行计算图，优化线上的权重（参数），得到模型。</li>
</ul>
<h3 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h3><ul>
<li><p>多维数组（列表）</p>
</li>
<li><p>根据中括号数判断维数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scalar = <span class="number">1</span></span><br><span class="line">vector = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">matrix = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line">tensor = [[[...]...]...]</span><br></pre></td></tr></table></figure>
</li>
<li><p>常量的声明</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]]) <span class="comment"># a matrix instead of a vector</span></span><br><span class="line">b = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]])</span><br><span class="line"></span><br><span class="line">result = tf.matmul(a, b)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><ul>
<li>只搭建，不计算。</li>
</ul>
<h3 id="会话-Session"><a href="#会话-Session" class="headerlink" title="会话(Session)"></a>会话(Session)</h3><ul>
<li><p>执行运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(y)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<ul>
<li><p>在TensorFlow1中，需要先建立计算图才能够计算。但是TensowFlow2中默认将两步合并，即<code>matmul</code>操作时就直接计算结果。</p>
</li>
<li><p>在2中调用1的方法/类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.compat.v1.Session() <span class="comment"># tf.compat.v1.xxx</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>运行时屏蔽TensorFlow的警告信息：在文件<strong>最前面</strong>（先于<code>import tensorflow</code>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li><p>神经网络线上的权重W</p>
</li>
<li><p>生成</p>
<ul>
<li><code>random_normal</code>：正态分布</li>
<li><code>truncated_normal</code>：去掉$2σ$以外的数据的正态分布</li>
<li><code>random_uniform</code>：平均分布</li>
<li><code>zeros</code></li>
<li><code>ones</code></li>
<li><code>fill</code>：全定值</li>
<li><code>constant</code>：直接给值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.compat.v1.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">2</span>, mean=<span class="number">0</span>, seed=<span class="number">1</span>))</span><br><span class="line">tf.zeros([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">tf.fill([<span class="number">3</span>, <span class="number">2</span>], <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="神经网络搭建"><a href="#神经网络搭建" class="headerlink" title="神经网络搭建"></a>神经网络搭建</h3><ul>
<li>准备数据集</li>
<li>搭建NN结构，从输入到输出。<ul>
<li>搭建计算图</li>
<li>绘画执行</li>
</ul>
</li>
<li>迭代优化NN参数</li>
<li>使用训练好的模型进行预测和分类</li>
</ul>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>模型搭建，实现推理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''两层简单全连接神经网络'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line">x = tf.constant([[<span class="number">0.7</span>, <span class="number">0.5</span>]])</span><br><span class="line">w1 = tf.Variable(tf.compat.v1.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.compat.v1.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op= tf.compat.v1.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>global_variables_initializer</code>返回<strong>初始化</strong>所有的<strong>全局变量</strong>(<code>Variable</code>)的操作，使用会话进行执行。</p>
<ul>
<li><p>全局变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(<span class="number">1</span>, name=<span class="string">'var_z'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>局部变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e = tf.Variable(<span class="number">6</span>, name=<span class="string">'var_e'</span>, collections=[tf.GraphKeys.LOCAL_VARIABLES])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>用<code>placeholder</code>实现<strong>输入</strong>定义</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.compat.v1.placeholder(tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>]]&#125;))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>输入多组数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.compat.v1.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>]]&#125;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>返回<strong>多组</strong>计算值（不只一个<code>y</code>）</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>训练模型参数，在所有参数上使用梯度下降，使NN模型在训练数据上的<strong>损失函数</strong>最小。</p>
<ul>
<li><p>损失函数：预测值(<code>y</code>)和已知答案(<code>y_</code>)之间的差距</p>
</li>
<li><p>均方差(MSE)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y_ - y))</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练方法：减小loss值为优化目标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line">tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)</span><br><span class="line">tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>learning_rate</code>：学习率，决定参数每次更新的幅度</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''两层简单全连接神经网络'''</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">seed = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机矩阵对象</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"><span class="comment"># 返回随机的 32 * 2 矩阵</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 将合法数据的结果置为1 否则为0</span></span><br><span class="line">Y = [[int(x0 + x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line">print(<span class="string">"X:"</span> + str(X))</span><br><span class="line">print(<span class="string">"Y:"</span> + str(Y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line">x = tf.compat.v1.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">w1 = tf.Variable(tf.compat.v1.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.compat.v1.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义标准输出</span></span><br><span class="line">y_ = tf.compat.v1.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和反向传播的方法</span></span><br><span class="line">loss = tf.compat.v1.reduce_mean(tf.square(y - y_))</span><br><span class="line">train_step = tf.compat.v1.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment"># 未经训练的参数值</span></span><br><span class="line">    print(<span class="string">'initial w1: '</span> + str(sess.run(w1)))</span><br><span class="line">    print(<span class="string">'initial w2: '</span> + str(sess.run(w2)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    <span class="comment"># 使用同一数据集的不同数据组合训练3000轮</span></span><br><span class="line">    STEPS = <span class="number">3000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        <span class="comment"># 传入y_指定标准输出</span></span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'final w1: '</span> + str(sess.run(w1)))</span><br><span class="line">    print(<span class="string">'final w2: '</span> + str(sess.run(w2)))</span><br></pre></td></tr></table></figure>
<h3 id="神经网络八股"><a href="#神经网络八股" class="headerlink" title="神经网络八股"></a>神经网络八股</h3><ul>
<li><p>准备</p>
<ul>
<li><code>import</code></li>
<li>常量定义</li>
<li>生成数据集</li>
</ul>
</li>
<li><p>前传</p>
<ul>
<li>定义输入、参数、标准输出</li>
</ul>
</li>
<li><p>反传</p>
<ul>
<li>定义损失函数和反传优化方法</li>
</ul>
</li>
<li><p>迭代</p>
<ul>
<li><p>生成会话，训练<code>STEPS</code>轮。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">3000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = ...</span><br><span class="line">        end = ...</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;...&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><ul>
<li><code>relu</code>:<code>tf.nn.relu()</code></li>
<li><code>sigmoid</code>:<code>tf.nn.sigmoid()</code></li>
<li><code>tanh</code>:<code>tf.nn.tanh()</code></li>
</ul>
<h3 id="NN复杂度"><a href="#NN复杂度" class="headerlink" title="NN复杂度"></a>NN复杂度</h3><ul>
<li>层数=隐藏层+输出层</li>
<li>总参数=总W+总b<ul>
<li><code>b</code>是偏置值，计算层一个结果神经元对应一个偏置。</li>
</ul>
</li>
</ul>
<h3 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">COST = <span class="number">1</span></span><br><span class="line">PROFIT = <span class="number">9</span></span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), COST * (y - y_), PROFIT * (y_ - y)))</span><br></pre></td></tr></table></figure>
<h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><ul>
<li><p>表征两个概率分布之间的<strong>距离</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ce = -tf.reduce_mean(y_ * tf.log(clip_by_value(y, <span class="number">1e-12</span>, <span class="number">1.0</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>将<code>n</code>分类的<code>n</code>个输出通过<code>softmax()</code>函数处理使其满足概率和为1的分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">cem = tf.reduce_mean(ce) <span class="comment"># loss function</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><ul>
<li>每次更新的幅度</li>
</ul>
<h3 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h3><ul>
<li><p>根据训练情况动态更新学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span>  <span class="comment"># 初始学习率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span>  <span class="comment"># 学习率衰减律</span></span><br><span class="line">LEARNING_RATE_STEP = <span class="number">1</span>  <span class="comment"># 学习率更新频度 一般为总样本数/BATCH_SIZE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 轮数计数器 初值为0 设置为不被训练</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 定义指数下降学习率</span></span><br><span class="line">learning_rate = tf.compat.v1.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=<span class="literal">True</span>)  <span class="comment"># 为True表示学习率的下降是梯度曲线</span></span><br><span class="line"><span class="comment"># 定义待优化参数</span></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = tf.square(w + <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 反向传播方法</span></span><br><span class="line">train_step = tf.compat.v1.train.GradientDescentOptimizer(</span><br><span class="line">    learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(w_val)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="滑动平均（影子值）"><a href="#滑动平均（影子值）" class="headerlink" title="滑动平均（影子值）"></a>滑动平均（影子值）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.compat.v1.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) <span class="comment"># 衰减率和当前轮数</span></span><br><span class="line">ema_op = ema.apply(tf.compat.v1.trainable_variables()) <span class="comment"># 对所有的待优化参数求平均值</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>) <span class="comment"># 将训练和滑动平均操作绑定</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>ema.average(arg)</code>：查看参数的滑动平均值</p>
</blockquote>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><ul>
<li>缓解过拟合：通过给某些参数权值，弱化训练数据的噪声，将其正则化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">30</span></span><br><span class="line">seed = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed产生随机数</span></span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line"><span class="comment"># 产生300组2维坐标</span></span><br><span class="line">X = rdm.randn(<span class="number">300</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 根据是否在园内为每个点设置颜色</span></span><br><span class="line">Y_ = [int(x0**<span class="number">2</span> + x1**<span class="number">2</span> &lt; <span class="number">2</span>) <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line">Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据集X和标签Y进行shape整理为矩阵</span></span><br><span class="line"><span class="comment"># 第一个参数-1表示属性值由第二个参数决定</span></span><br><span class="line"><span class="comment"># 第二个参数表示有多少列 即X为n行2列 Y为n行1列</span></span><br><span class="line">X = np.vstack(X).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取X所有坐标的第0/1列元素并指定颜色进行绘制</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入 参数 输出 和前向传播过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.compat.v1.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.compat.v1.add_to_collection(</span><br><span class="line">        <span class="string">'losses'</span>,</span><br><span class="line">        tf.keras.regularizers.l2(regularizer)(w))</span><br><span class="line">        <span class="comment"># tf.contrib.layers.l2_regularizer(regularizer)(w))</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.compat.v1.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.compat.v1.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([<span class="number">2</span>, <span class="number">11</span>], <span class="number">0.01</span>)</span><br><span class="line">b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x, w1) + b1)  <span class="comment"># 激活隐藏层</span></span><br><span class="line"></span><br><span class="line">w2 = get_weight([<span class="number">11</span>, <span class="number">1</span>], <span class="number">0.01</span>)</span><br><span class="line">b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">y = tf.matmul(y1, w2) + b2  <span class="comment"># 输出层不激活</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line">loss_total = loss_mse + tf.compat.v1.add_n(</span><br><span class="line">    tf.compat.v1.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不含正则化的反向传播</span></span><br><span class="line">train_step = tf.compat.v1.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 包含正则化的反向传播</span></span><br><span class="line">train_step = tf.compat.v1.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">40000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            loss_mse_v = sess.run(loss_mse, feed_dict=&#123;x: X, y_: Y_&#125;)</span><br><span class="line">    <span class="comment"># 在[-3,3]上以0.01为步长生成二维网格坐标点</span></span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">    <span class="comment"># 将xx, yy拉直，合并成2列的矩阵</span></span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    <span class="comment"># 将点喂入神经网络</span></span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x: grid&#125;)</span><br><span class="line">    <span class="comment"># 调成probs的shape为和xx相同</span></span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="搭建八股"><a href="#搭建八股" class="headerlink" title="搭建八股"></a>搭建八股</h2><ul>
<li><p>生成数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">()</span>:</span></span><br><span class="line">    rdm = np.random.RandomState(seed)</span><br><span class="line">    X = </span><br><span class="line">    Y_ = </span><br><span class="line">    Y_c = </span><br><span class="line">    </span><br><span class="line">    X = np.vstack(X).reshape()</span><br><span class="line">    Y = np.vstack(Y).reshape()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y_, Y_c</span><br></pre></td></tr></table></figure>
</li>
<li><p>前向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''forward.py'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w = </span><br><span class="line">    b = </span><br><span class="line">    y = </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable()</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf,contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable()</span><br><span class="line">    <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure>
</li>
<li><p>反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''backward.py'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.palceholer()</span><br><span class="line">    y_ = tf.palceholer()</span><br><span class="line">    y = forward.firward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">    loss = </span><br><span class="line">	</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x:, y_:&#125;)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><ul>
<li><code>tf.get_collection(&#39;&#39;)</code>：取<code>collection</code>中的所有变量并生成一个列表</li>
<li><code>tf.add_n([])</code>：将列表中对应的元素相加</li>
<li><code>tf.cast(x, dtype)</code>：将<code>x</code>的类型转换为<code>dtype</code></li>
<li><code>tf.argmax(x, axis)</code>：返回最大值所在的索引号</li>
<li><code>os.path.join(&#39;home&#39;, &#39;name&#39;)</code>：返回<code>home/name</code></li>
<li><code>with tf.Graph().as_default() as g:</code>：语句块中的节点在计算图<code>g</code>中（多用于<strong>复现</strong>神经网络）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''mnist_forward.py'''</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    <span class="comment">#beijieduande biaozhuncha buhuichaoguo liangge biaozhuncha</span></span><br><span class="line">    w = tf.Variable(tf.compat.v1.truncated_normal(shape, stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="keyword">if</span> regularizer != <span class="literal">None</span>:</span><br><span class="line">        tf.compat.v1.add_to_collection(</span><br><span class="line">            <span class="string">"losses"</span>,</span><br><span class="line">            tf.keras.regularizers.l2(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable(tf.zeros(shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w1 = get_weight((INPUT_NODE, LAYER_NODE), regularizer)</span><br><span class="line">    b1 = get_bias(LAYER_NODE)</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight((LAYER_NODE, OUTPUT_NODE), regularizer)</span><br><span class="line">    b2 = get_bias(OUTPUT_NODE)</span><br><span class="line">    y2 = tf.matmul(y1, w2) + b2</span><br><span class="line">    <span class="keyword">return</span> y2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.random.random((<span class="number">3</span>, <span class="number">784</span>))</span><br><span class="line">    x = x.astype(np.float32)</span><br><span class="line">    print(<span class="string">"x.shape:"</span>, x.shape)</span><br><span class="line">    regularizer = <span class="number">0.001</span></span><br><span class="line">    print(x.dtype)</span><br><span class="line"></span><br><span class="line">    pred_y = forward(x, regularizer)</span><br><span class="line">    print(<span class="string">"pred_y:"</span>, pred_y[:])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''mnist_backward.py'''</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line">REGULARIZER = <span class="number">0.0001</span></span><br><span class="line">STEPS = <span class="number">50000</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line">MODEL_SAVE_PATH = <span class="string">"./model/"</span></span><br><span class="line">MODEL_NAME = <span class="string">'mnist_model'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(mnist)</span>:</span></span><br><span class="line"></span><br><span class="line">    x = tf.compat.v1.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">    y_ = tf.compat.v1.placeholder(tf.float32,</span><br><span class="line">                                  [<span class="literal">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">    y = mnist_forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,</span><br><span class="line">                                                        labels=tf.argmax(</span><br><span class="line">                                                            y_, <span class="number">1</span>))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.compat.v1.get_collection(<span class="string">"losses"</span>))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.compat.v1.train.exponential_decay(LEARNING_RATE_BASE,</span><br><span class="line">                                               global_step,</span><br><span class="line">                                               mnist.train.num_examples /</span><br><span class="line">                                               BATCH_SIZE,</span><br><span class="line">                                               LEARNING_RATE_DECAY,</span><br><span class="line">                                               staircase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_step = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(</span><br><span class="line">        loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    ema = tf.compat.v1.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.compat.v1.trainable_variables())</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">    saver = tf.compat.v1.train.Saver()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        ckpt = tf.compat.v1.train.get_checkpoint_state(MODEL_SAVE_PATH)</span><br><span class="line">        exclude = [<span class="string">''</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">            saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step],</span><br><span class="line">                                           feed_dict=&#123;</span><br><span class="line">                                               x: xs,</span><br><span class="line">                                               y_: ys</span><br><span class="line">                                           &#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                print(</span><br><span class="line">                    <span class="string">"After %d training step(s), losses on trianing batch is %f. "</span></span><br><span class="line">                    % (step, loss_value))</span><br><span class="line">                saver.save(sess,</span><br><span class="line">                           os.path.join(MODEL_SAVE_PATH, MODEL_NAME),</span><br><span class="line">                           global_step=global_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''mnist_test.py'''</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> mnist_backward</span><br><span class="line">TEST_INTERVAL_SECS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">        x = tf.compat.v1.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">        y_ = tf.compat.v1.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">        y = mnist_forward.forward(x, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        ema = tf.compat.v1.train.ExponentialMovingAverage(</span><br><span class="line">            mnist_backward.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.compat.v1.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        <span class="comment">#qiupingjun</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> j &lt; <span class="number">20</span>:</span><br><span class="line">            <span class="comment">#gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)</span></span><br><span class="line">            <span class="comment">#with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:</span></span><br><span class="line">            <span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">                ckpt = tf.compat.v1.train.get_checkpoint_state(</span><br><span class="line">                    mnist_backward.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="comment">#                print("----")</span></span><br><span class="line">                <span class="comment">#                print(ckpt)</span></span><br><span class="line">                <span class="comment">#                print("----")</span></span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    <span class="comment">#                    print("ok?")</span></span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(</span><br><span class="line">                        <span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</span><br><span class="line">                    <span class="comment">#                    print("global_step:",global_step)</span></span><br><span class="line">                    accuracy_score = sess.run(accuracy,</span><br><span class="line">                                              feed_dict=&#123;</span><br><span class="line">                                                  x: mnist.test.images[:<span class="number">256</span>],</span><br><span class="line">                                                  y_: mnist.test.labels[:<span class="number">256</span>]</span><br><span class="line">                                              &#125;)</span><br><span class="line">                    print(<span class="string">"After %s training step(s). test accuracy = %f"</span> %</span><br><span class="line">                          (global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">"No checkpoint file found"</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''mnist_app.py'''</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Nov 28 22:37:56 2018</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: lele</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> mnist_backward</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"></span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_model</span><span class="params">(testPicArr)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">        x = tf.compat.v1.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">        y = mnist_forward.forward(x, <span class="literal">None</span>)</span><br><span class="line">        preValue = tf.compat.v1.arg_max(y, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        variable_averages = tf.compat.v1.train.ExponentialMovingAverage(</span><br><span class="line">            mnist_backward.MOVING_AVERAGE_DECAY)</span><br><span class="line">        variable_to_restore = variable_averages.variables_to_restore()</span><br><span class="line">        saver = tf.compat.v1.train.Saver(variable_to_restore)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            ckpt = tf.compat.v1.train.get_checkpoint_state(</span><br><span class="line">                mnist_backward.MODEL_SAVE_PATH)</span><br><span class="line">            <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">                preValue = sess.run(preValue, feed_dict=&#123;x: testPicArr&#125;)</span><br><span class="line">                <span class="keyword">return</span> preValue</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"No checkpoint file found"</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pre_pic</span><span class="params">(picName)</span>:</span></span><br><span class="line">    fp = open(picName, <span class="string">'rb'</span>)</span><br><span class="line">    img = Image.open(fp)</span><br><span class="line">    reIm = img.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)</span><br><span class="line">    im_arr = np.array(reIm.convert(<span class="string">'L'</span>))</span><br><span class="line">    threshold = <span class="number">50</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">            im_arr[i][j] = <span class="number">255</span> - im_arr[i][j]</span><br><span class="line">            <span class="keyword">if</span> (im_arr[i][j] &lt; threshold):</span><br><span class="line">                im_arr[i][j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                im_arr[i][j] = <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    nm_arr = im_arr.reshape([<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">    nm_arr = nm_arr.astype(np.float32)</span><br><span class="line">    img_ready = np.multiply(nm_arr, <span class="number">1.0</span> / <span class="number">255.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img_ready</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">application</span><span class="params">()</span>:</span></span><br><span class="line">    testNum = int(input(<span class="string">"input the num of the test pictures:"</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(testNum):</span><br><span class="line">        testPic = input(<span class="string">"the path of test picture:"</span>)</span><br><span class="line">        testPicArr = pre_pic(testPic)</span><br><span class="line">        preValue = restore_model(testPicArr)</span><br><span class="line">        print(<span class="string">"the prediction num is:"</span>, preValue)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    application()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><ul>
<li><p>待优化参数过多会导致过拟合</p>
</li>
<li><p>为了避免过拟合，通常先将图片进行预处理（提取特征）。</p>
</li>
<li><p>卷积是一种有效的<strong>提取图像特征</strong>的方法</p>
</li>
<li><p>$输出图片边长=(输入图片边长-卷积核长+1)/步长(向上取整)$</p>
<blockquote>
<p>为了将输出图片和输入图片的边长保持一致，可以对输入图片进行<strong>全零填充(Padding)</strong>。</p>
</blockquote>
</li>
</ul>
<h3 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h3><ul>
<li><p><code>tf.nn.conv2d</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d([batch, <span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">16</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>padding</code><ul>
<li><code>VALID</code>：不进行填充</li>
<li><code>SAME</code>：进行填充</li>
</ul>
</li>
<li>核个数指定<strong>输出</strong>的通道数</li>
</ul>
</li>
</ul>
<h3 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化(Pooling)"></a>池化(Pooling)</h3><ul>
<li><p>减少特征数量（模糊处理）</p>
</li>
<li><p>最大值池化：提取图片纹理</p>
</li>
<li><p>均值池化：保留背景特征</p>
</li>
<li><p>计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pool = tf.nn.max_pool([batch, <span class="number">28</span>, <span class="number">28</span>, <span class="number">6</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) <span class="comment"># tf.nn.avg_pool</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="舍弃-Dropout"><a href="#舍弃-Dropout" class="headerlink" title="舍弃(Dropout)"></a>舍弃(Dropout)</h3><ul>
<li><p>训练时随机舍弃一些神经元，但是在使用时<strong>会恢复</strong>。</p>
</li>
<li><p>实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(上层输出， 舍弃的概率)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>一般放在<strong>全连接</strong>网络中</p>
</blockquote>
</li>
</ul>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ul>
<li>卷积</li>
<li>激活</li>
<li>池化</li>
<li>全连接</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>TensorFlow Notebook, Peking University, 2018.</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/23/Python%E5%8F%98%E9%87%8F%E8%AF%A6%E8%A7%A3/" rel="prev" title="Python变量详解">
      <i class="fa fa-chevron-left"></i> Python变量详解
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Silence</span>
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  Visitors: <span id="busuanzi_value_site_pv"></span>
</span>
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">|   total: 65.4k words</span>
</div>
  <div class="powered-by">
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    new Valine(Object.assign({
      el  : '#valine-comments',
      path: location.pathname,
    }, {"enable":true,"appId":"GMneVEFPu4TwpvejAPqllWGc-gzGzoHsz","appKey":"5eNvc7SV1dxRT000WSXjRDuD","placeholder":"Just write what you'd like to write.","avatar":"mm","meta":["nick","mail"],"pageSize":10,"language":"en","visitor":false,"comment_count":true,"recordIP":false,"serverURLs":null}
    ));
  }, window.Valine);
});
</script>

</body>
</html>

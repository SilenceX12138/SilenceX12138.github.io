<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/lib/animate-css/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"8.0.0-rc.4","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}}};
  </script>

  <meta name="description" content="Learning note of 2021 NTU Machine Learning Course.">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Course Note">
<meta property="og:url" content="http://yoursite.com/2021/06/23/Machine-Learning-Course-Note/index.html">
<meta property="og:site_name" content="Silence">
<meta property="og:description" content="Learning note of 2021 NTU Machine Learning Course.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/06/23/5vVFz2KRG7rIny3.png">
<meta property="og:image" content="https://i.loli.net/2021/06/23/o3AQOKPEci6wWHC.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-d3ccd01453f215cf3357192debd14489_1440w.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-083ca0bcd0749fd0f236a690b50442e6_1440w.png">
<meta property="og:image" content="https://i.loli.net/2021/05/18/1VFZxoh5P6EGjBN.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-74f7d2430ace69c9cd6b963eb58a5079_1440w.jpg">
<meta property="og:image" content="https://i.loli.net/2021/05/18/cq1yjMaveILkTzd.png">
<meta property="og:image" content="https://i.loli.net/2021/05/18/WXJ4PCbVIctwEsS.png">
<meta property="article:published_time" content="2021-06-23T12:49:05.050Z">
<meta property="article:modified_time" content="2021-06-24T00:26:09.817Z">
<meta property="article:author" content="Silence">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/06/23/5vVFz2KRG7rIny3.png">

<link rel="canonical" href="http://yoursite.com/2021/06/23/Machine-Learning-Course-Note/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Machine Learning Course Note | Silence</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Silence" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Silence</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-lover">

    <a href="/lover/" rel="section"><i class="fa fa-heart fa-fw"></i>lover</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
  </ul>
</nav>




</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Concepts"><span class="nav-number">1.</span> <span class="nav-text">Basic Concepts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification"><span class="nav-number">2.</span> <span class="nav-text">Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">3.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation"><span class="nav-number">4.</span> <span class="nav-text">Backpropagation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Support-Vector-Machine"><span class="nav-number">5.</span> <span class="nav-text">Support Vector Machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning"><span class="nav-number">6.</span> <span class="nav-text">Deep Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolutional-Neural-Network"><span class="nav-number">7.</span> <span class="nav-text">Convolutional Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Self-attention"><span class="nav-number">8.</span> <span class="nav-text">Self-attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Network"><span class="nav-number">9.</span> <span class="nav-text">Recurrent Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Semi-supervised-Learning"><span class="nav-number">10.</span> <span class="nav-text">Semi-supervised Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised-Learning"><span class="nav-number">11.</span> <span class="nav-text">Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster"><span class="nav-number">11.1.</span> <span class="nav-text">Cluster</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PCA"><span class="nav-number">11.2.</span> <span class="nav-text">PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVD"><span class="nav-number">11.3.</span> <span class="nav-text">SVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Enbedding"><span class="nav-number">11.4.</span> <span class="nav-text">Word Enbedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Explainable-Machine-Learning"><span class="nav-number">12.</span> <span class="nav-text">Explainable Machine Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attack-ML-Model"><span class="nav-number">13.</span> <span class="nav-text">Attack ML Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Network-Compression"><span class="nav-number">14.</span> <span class="nav-text">Network Compression</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Silence"
      src="https://i.loli.net/2020/07/05/Ml7X4UshoCJgn52.jpg">
  <p class="site-author-name" itemprop="name">Silence</p>
  <div class="site-description" itemprop="description">I'm right here waiting for you.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/SilenceX12138" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SilenceX12138" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/silencejiang12138" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;silencejiang12138" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/06/23/Machine-Learning-Course-Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/07/05/Ml7X4UshoCJgn52.jpg">
      <meta itemprop="name" content="Silence">
      <meta itemprop="description" content="I'm right here waiting for you.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Silence">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning Course Note
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-23 20:49:05" itemprop="dateCreated datePublished" datetime="2021-06-23T20:49:05+08:00">2021-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-24 08:26:09" itemprop="dateModified" datetime="2021-06-24T08:26:09+08:00">2021-06-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2021/06/23/Machine-Learning-Course-Note/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/23/Machine-Learning-Course-Note/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>Learning note of 2021 NTU Machine Learning Course.</strong></p>
<a id="more"></a>
<h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><ul>
<li><p>当训练集loss小而测试集loss大时，<strong>不一定是</strong>过拟合，也有可能是因为训练集和测试集数据<strong>分布不同</strong>而造成的mismatch.</p>
</li>
<li><p>神经网络在训练时会有随机因素影响参数的更新，但是在训练结束后应当具有稳定的输出。</p>
</li>
<li><p>数据增强需要根据数据本身的特性进行，例如图像通常会进行左右翻转而<strong>不是</strong>上下翻转。</p>
</li>
<li><p>模型训练时遇到的梯度为0的点成为critical point，分为局部最小值、局部最大值和鞍点，其中<strong>鞍点最为常见</strong>。</p>
<blockquote>
<p>出现鞍点时，理论上可以通过计算Hessian矩阵的特征值，根据特征值为负的向量来更新参数，但实际操作时由于计算量较大而<strong>不采用</strong>。</p>
</blockquote>
</li>
<li><p><strong>最小化</strong>交叉熵等价于<strong>最大化</strong>似然函数</p>
<ul>
<li>交叉熵：衡量两个分布之间的差异</li>
<li>似然：模型对样本分布的解释力度</li>
</ul>
</li>
</ul>
<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><ul>
<li><p>Naïve Bayes</p>
<ul>
<li><p>判别模型&amp;生成模型：<a href="https://www.zhihu.com/question/20446337/answer/256466823" target="_blank" rel="noopener">https://www.zhihu.com/question/20446337/answer/256466823</a></p>
<blockquote>
<ul>
<li>判别模型学习区别，生成模型学习本质。</li>
<li>判别模型直接对$P(C|x)$进行建模，而生成模型先求$P(C,x)$.</li>
<li>判别模型力求<strong>经验误差最小化</strong>，生成模型强调<strong>出现概率最大化</strong>。</li>
</ul>
</blockquote>
</li>
<li><p>“拉普拉斯修正”平滑操作可以避免信息被训练集中未出现的属性值“抹去”</p>
<blockquote>
<p>假设属性值与类别均匀分布</p>
</blockquote>
</li>
<li><p>联合概率：包含多个条件且<strong>所有条件同时成立</strong>的概率，记作$P(X=a,Y=b)$或$P(a,b)$.</p>
</li>
</ul>
</li>
<li><p>参数估计：设$p(x)=f(\theta)$即$x$的概率分布与$\theta$相关</p>
<ul>
<li><p>极大似然：$argmax\{P(x|\theta)\}$，取能够使概率值最大的参数$\theta$.</p>
<blockquote>
<p>$P(x|\theta)$实际上就是将$\theta$代入假设的分布后得到的函数</p>
</blockquote>
</li>
<li><p>贝叶斯：在$p(x)$分布的基础上，设出第二个参数的分布$g(\theta)$.通过贝叶斯公式估计$g(\theta)$，然后求出期望平均作为$\theta$的估计值，然后再求出$p(x)$</p>
</li>
<li><p>最大后验概率：$argmax\{P(x|\theta)P(\theta)\}$，即在极大似然的基础上增加对参数本身概率的考量。（$P(\theta)$是对数据的<strong>主观认识</strong>，即人为设置其概率分布）</p>
</li>
</ul>
</li>
<li><p>$\hat{y}$在李宏毅老师MOOC中表示<strong>真实值</strong>，而西瓜书中是<strong>预测值</strong>。</p>
</li>
</ul>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><ul>
<li><p>Logistic Regression是在线性回归的基础上增加了<code>sigmoid</code>函数，解决的是<strong>分类问题</strong>。</p>
</li>
<li><p>偏置项用于增强模型的拟合能力：<a href="https://www.zhihu.com/question/305340182/answer/721739423" target="_blank" rel="noopener">https://www.zhihu.com/question/305340182/answer/721739423</a></p>
<blockquote>
<p>偏置也可以理解成<strong>阈值</strong>，只有当计算出的值足够大时（大过阈值），才表示决策更加笃定。</p>
</blockquote>
</li>
<li><p>Logistic Regression使用的误差表示方法是<strong>交叉熵</strong>，因为MSE会出现label处梯度为0的情况，不能保证优化参数。</p>
</li>
<li><p>$y=w^Tx+b$越大越好的原因：损失函数中想要使$f(y)$在正样本时尽量向<code>1</code>靠近，也就是要求$y$的值尽量大，所以$y$越大则表示样本分类为正的可能性越大。</p>
<blockquote>
<p>如果正样本时要求$f(y)$向<code>-1</code>靠近，则$y$的值就应该越小越好。</p>
</blockquote>
</li>
<li><p>通过多个Logistic Regressor的拼接，可以构成神经网络。</p>
<blockquote>
<ul>
<li>本质上是用Logistic Regression来寻找<strong>最好</strong>的特征</li>
<li>最后的输出层实际上是<code>OvR</code>模型</li>
</ul>
</blockquote>
</li>
<li><p>softmax的作用：将不同类别的预测值归一化，同时将差异<strong>放大</strong>。</p>
<p><img src="https://i.loli.net/2021/06/23/5vVFz2KRG7rIny3.png" alt="img" style="zoom: 50%;" /></p>
<blockquote>
<p>同时将<strong>负</strong>预测值转换为<strong>正值</strong></p>
</blockquote>
</li>
<li><p>梯度下降方法是<strong>提前</strong>将偏微分公式求出，然后带入实际的参数（权值、偏置）进行计算得到梯度值，然后更新参数，例子如下：$\left(x_{2}\right)=\left(x_{1}\right)-\eta \nabla f\left(x_{1}\right)=(6)-0.2 \times(12)=(3.6)$.</p>
</li>
</ul>
<h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><ul>
<li><p>反向传播是一种高效的计算<strong>损失函数</strong>相对于参数<strong>梯度</strong>的办法</p>
</li>
<li><p>参数更新方程：<script type="math/tex">\cases {w_i^2=w_i^1-\Delta w_i \\ \Delta w_i=\eta*\frac{\partial L}{\partial w_i}}</script></p>
<blockquote>
<ul>
<li>更新方向和梯度方向<strong>相反</strong></li>
<li>存在多种不同的更新方式（优化器）</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial C}{\partial w_n}=\frac{\partial C}{\partial z}*\frac{\partial z}{\partial w_n}$</p>
<ul>
<li>$C$(cost)是某一个样本的预测值和真实值的<strong>距离</strong></li>
<li>$z$是$w_n$对应神经元的输入值线性回归（<strong>没有</strong>经过激活函数）</li>
<li>损失函数是多个$C$的累加</li>
</ul>
</li>
<li><p>$\frac{\partial z}{\partial w_n}=x_n$：<code>Forward Pass</code>中，单层神经元的输出对<strong>权重</strong>求导是对应的<strong>输入特征</strong>的值。</p>
</li>
<li><p>$\frac{\partial C}{\partial z}$：<code>Backward Pass</code>本质上和正向的<code>Forward Pass</code>相同，即建立一个反向的神经网络进行正常的<strong>预测</strong>计算。</p>
<ul>
<li>反向网络中神经元的激活函数是原激活函数的<strong>导函数</strong></li>
<li>反向网络的激活函数直接和线性回归方程<strong>相乘</strong>，而<strong>不是复合</strong>。</li>
</ul>
</li>
<li><p><code>Sigmoid</code>函数可能导致梯度消失：<strong>输出端</strong>参数收敛，<strong>输入端</strong>参数依然处在随机态。</p>
</li>
</ul>
<h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><ul>
<li><p>SVM对噪声敏感</p>
</li>
<li><p>硬间隔：$\hat{y}^if(x_i)\geq 1$</p>
</li>
<li><p>软间隔：</p>
<ul>
<li><p>目标函数：$min\ \frac{||w||_2}{2}+C\Sigma l(\hat{y}^if(x^i)-1)$</p>
<ul>
<li><p>$C$：常数，无穷大时<strong>不允许</strong>错误出现，软间隔变为硬间隔。</p>
</li>
<li><p>$l(x)$：<code>0/1</code>损失函数，当$x&lt;0$时，取值为1.通常使用<code>hinge loss</code>函数($l(z)=max\{0,1-z\}$)代替</p>
<blockquote>
<p><code>hinge loss</code>是$l(x)$的<strong>紧凑上界</strong></p>
</blockquote>
</li>
</ul>
</li>
<li><p>约束：$\hat{y}^if(x^i)\geq 1-\epsilon^i(\epsilon^i \geq 0)$</p>
</li>
<li>求解<ul>
<li>拉格朗日乘子法+SMO</li>
<li>线性SVM：<strong>梯度下降</strong>（间断函数如<code>hinge loss</code>也是可以使用梯度下降进行优化的）</li>
</ul>
</li>
</ul>
<blockquote>
<p>在使用<code>hinge loss</code>函数时，$\epsilon ^i=max\{0,1-\hat{y}^if(y^i)\}$，即满足非负约束和大于$1-\hat{y}^if(x^i)$的条件下，令$\Sigma \epsilon^i$尽量小，从而使目标函数最小。</p>
</blockquote>
</li>
<li><p>核函数</p>
<ul>
<li>让特征在高维空间<strong>线性可分</strong>，从而得到一个非线性SVM.（普通支持向量机<strong>只能解决线性可分</strong>问题，此时SVM可以处理低维空间线性不可分的数据）</li>
<li>降低运算量，在计算高维空间的目标函数时不需要真正做变换，只需要计算核函数即可。</li>
</ul>
</li>
<li><p>SVM/SVR学习出的模型都可以表示成<strong>核函数的线性组合</strong>：$h^*(x)=\Sigma\alpha_i K(x,x_i)$</p>
<ul>
<li>模型仅与支持向量有关，<strong>不用记录</strong>其他样本。</li>
<li>偏置项被合并进了支持向量</li>
</ul>
</li>
</ul>
<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><ul>
<li><p>同样的参数量，深层神经网络比只有一层的感知机性能更好。</p>
<ul>
<li>深层神经网络蕴含了“模块化”的思想，例如各层网络进行不同维度的分类。</li>
<li>因为每个模块的分类功能并不复杂，神经网络需要的训练数据实际上是<strong>更少</strong>的。</li>
</ul>
</li>
<li><p>End-to-end Learning：让流水线上每一个函数自己学习相应的特征</p>
</li>
<li><p>训练指南</p>
<ul>
<li><p>过拟合的<strong>前提</strong>是训练集的表现很好，如果训练集就没有高性能，那么不叫过拟合。</p>
</li>
<li><p>训练集</p>
<ul>
<li>更换激活函数<ul>
<li>ReLU函数可以<strong>防止</strong>梯度消失：有效的神经元是线性的，不会出现极小的梯度。</li>
</ul>
</li>
<li>调整学习率</li>
</ul>
</li>
<li><p>测试集</p>
<ul>
<li><p>及时停止训练：根据<strong>验证集</strong>确定时机</p>
<ul>
<li><p>验证集(fold 5)&amp;测试集</p>
<p><img src="https://i.loli.net/2021/06/23/o3AQOKPEci6wWHC.png" alt="img"></p>
<ul>
<li>验证集用于模型筛选、调优和消融实验</li>
</ul>
</li>
</ul>
</li>
<li><p>测试集<strong>仅</strong>用于评估模型性能</p>
</li>
<li><p>正则化：正则项有多种形式，例如L1/L2.</p>
<blockquote>
<ul>
<li>正则项中通常<strong>不含</strong>偏置项</li>
<li>正则化目的是让参数尽量靠近零，某些情况下效果<strong>不如</strong>停止训练显著。</li>
</ul>
</blockquote>
</li>
<li><p>Dropout：在<strong>训练</strong>过程中<strong>每次</strong>更新参数时，以概率值$p$随机<strong>舍弃</strong>一些神经元。（激活函数以概率$p$输出0）</p>
<ul>
<li>rescale：预测时<strong>所有神经元</strong>都会参与，输出是朴素情况$\frac{1}{1-p}$倍，因此需要乘$(1-p)$保证规模正常。</li>
</ul>
</li>
<li><p>inverted dropout：在训练时将<strong>输出缩小</strong>为$(1-p)$，测试时就<strong>不需要</strong>rescale了。</p>
</li>
<li><p>Batch Normalization：在激励函数前对数据特征值进行标准化，可以提高模型和性能和学习速度。</p>
<p><img src="https://pic2.zhimg.com/80/v2-d3ccd01453f215cf3357192debd14489_1440w.png" alt="img" style="zoom:33%;" /></p>
<ul>
<li><p>在BN操作外还需要添加<strong>反标准化</strong>，从而使神经网络自行对BN有效性进行判断，即学习参数$\gamma$和$\beta$.</p>
<p><img src="https://pic3.zhimg.com/80/v2-083ca0bcd0749fd0f236a690b50442e6_1440w.png" alt="img" style="zoom:33%;" /></p>
</li>
<li><p>BN操作本质上是使<strong>error surface平滑</strong>，避免模型困在critical point.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>SGD&amp;标准GD$\Leftrightarrow$标准BP&amp;累计BP，即单个样本&amp;batch中的样本。</p>
</li>
</ul>
<h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><ul>
<li>卷积的作用<ul>
<li>特征的位置对特征的识别没有影响</li>
<li>特征可能只在原图中的很小一部分</li>
</ul>
</li>
<li>池化的作用<ul>
<li>缩放原图不影响特征的信息</li>
</ul>
</li>
<li>卷积的本质<ul>
<li>权重是卷积的<strong>参数</strong></li>
<li>全连接网络<strong>舍弃</strong>一部分权重</li>
<li>卷积层之后的神经元<strong>共享</strong>权重</li>
</ul>
</li>
<li>卷积核的叠加：10个$1<em>3</em>3$的卷积核将图像升维到后，下一个卷积核实际上是<strong>三维</strong>的，即$10<em>3</em>3$.此时如果再将图像升维，则需要<strong>多个</strong>三维卷积核。</li>
<li>特征图中的<strong>一个像素</strong>实际上就是<strong>一个神经元</strong>（<strong>不包括</strong>激活函数）</li>
<li>二维卷积&amp;三维卷积<ul>
<li><code>conv2D</code>：可以具有多个通道，但是每个通道是<strong>二维</strong>的，例如图像处理。</li>
<li><code>conv3D</code>：每个通道是<strong>三维</strong>的，例如CT影像和视频处理。</li>
</ul>
</li>
<li>感受野(receptive field)可以是<strong>长方形</strong>的</li>
</ul>
<h2 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h2><ul>
<li><p>作用：在输入为<strong>序列</strong>时对其进行预处理，即通过考虑整个输入的上下文信息对输入进行处理。</p>
<blockquote>
<ul>
<li>将图片某个像素的多个通道抽象为一个vector，也可以把整张图当作m*n个向量的序列。</li>
<li>可以在神经网络<strong>隐层</strong>中使用</li>
<li>现在通常使用self-attention替代RNN</li>
</ul>
</blockquote>
</li>
<li><p>卷积实际上就是简化的self-attention</p>
</li>
<li><p>positional encoding：弥补self-attention没有考虑位置信息的问题</p>
</li>
</ul>
<h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><ul>
<li><p>LSTM是采用RNN思想的另一种<strong>网络架构</strong>（并不是神经元）</p>
</li>
<li><p>RNN通常<strong>不使用</strong><code>ReLu</code>作为激活函数</p>
</li>
<li><p>LSTM中记忆单元存储的值在遗忘门没有打开时，更新操作适合输入流相加，而不是RNN的直接替换，所以能够保证：所有对记忆单元造成了影响的值不会被完全遗忘，从而解决了RNN的梯度消失问题。</p>
<blockquote>
<ul>
<li>梯度消失的本质：距离较远的时间序列的信息难以在参数更新拥有话语权</li>
<li>LSTM仍然可能出现梯度爆炸，因此学习率通常设置的很小。</li>
</ul>
</blockquote>
</li>
<li><p>GRU将输入门和遗忘门联系起来（只剩下两个门），降低了参数量。</p>
</li>
<li><p>RNN下一状态隐层输出：$h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh}) $</p>
<ul>
<li><p>需要学习的参数：输入和记忆单元传递使用的权值$W$，两个偏置。</p>
<p><img src="https://i.loli.net/2021/05/18/1VFZxoh5P6EGjBN.png" alt="image-20210518110154693" style="zoom: 50%;" /></p>
</li>
</ul>
</li>
</ul>
<h2 id="Semi-supervised-Learning"><a href="#Semi-supervised-Learning" class="headerlink" title="Semi-supervised Learning"></a>Semi-supervised Learning</h2><ul>
<li><p>纯半监督学习的测试数据<strong>不是</strong>未标注数据，直推学习(transductive)的<strong>测试数据</strong>就是<strong>未标注数据</strong>。</p>
</li>
<li><p>主动学习：逐步扩展有标注数据集（专家协助），力求使用最小的数据量进行训练。</p>
<blockquote>
<p>增加了标注数据量，<strong>不属于</strong>半监督学习。</p>
</blockquote>
</li>
<li><p>生成式模型：基于极大似然思想，代表方法为$EM$法.</p>
<ul>
<li><p><strong>似然</strong>是用样本推测分布（参数），<strong>概率</strong>是用分布（参数）对样本进行预测。</p>
</li>
<li><p><strong>需要先对样本的分布做出假设</strong></p>
</li>
<li><p>每个样本既可以是正类，也可以是负类。</p>
</li>
<li><p>优化函数：$argmax\ \Sigma (P(x|C1)P(C1)+P(x|C2)P(C2))$</p>
<blockquote>
<p>概率值与参数$\theta$有关</p>
</blockquote>
</li>
</ul>
</li>
<li><p><code>Low Density</code>假设：在不同类别的分界处出现样本的概率是很小的（非黑即白）</p>
<ul>
<li>self-learning：不同于主动学习，模型使用<code>pseudo label</code>进行更新。</li>
<li>S3VM</li>
<li>正则化：无标注数据交叉熵（使预测是一个偏态分布）</li>
</ul>
<blockquote>
<p><strong>不</strong>适用于<strong>回归</strong>问题：回归问题的输出是确定值而不是概率，即重新输入模型的值和模型的原输出值相同，因此生成的pseudo value被加入标注数据集<strong>并不能</strong>优化原有参数。</p>
</blockquote>
</li>
<li><p><code>Smoothness</code>假设：相同类别的样本的模型输出应该是相近的</p>
<ul>
<li>Cluster</li>
<li>Graph based</li>
<li>正则化：无标注数据平滑度（保证联系够紧密的才能分为一类）</li>
</ul>
</li>
</ul>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><p>无监督学习通常应用在数据挖掘领域，在没有标签的数据中找寻内在规律并解决问题。</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><ul>
<li><p>聚类可以单独解决一个问题，也可以作为其他方法的<strong>预处理</strong>手段。</p>
</li>
<li><p>聚类最常使用的方法：k-means</p>
<blockquote>
<p><code>kNN</code>（k近邻）算法：懒惰监督学习，根据输入计算出距离最近的<code>k</code>个训练样本，然后取最多的标签作为预测结果。</p>
</blockquote>
</li>
</ul>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><ul>
<li><p>通过提取主成分，在丢失<strong>少量信息</strong>的情况下，<strong>大幅度降低</strong>运算复杂度。</p>
</li>
<li><p>线性降维方法：通过<strong>线性变换</strong>（矩阵运算）减少特征维数</p>
</li>
<li><p>主成分本质上就是原成分构成的一组新基</p>
<ul>
<li><p>范数（模长）为1</p>
</li>
<li><p>相互正交（协方差为零/不相关）</p>
</li>
<li><p>新基是原特征间协方差矩阵的特征向量和原特征的内积，按照特征值大小<strong>降序排列</strong>。</p>
<blockquote>
<p>特征值越大，说明该成分对样本特征影响越大（越重要）。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>PCA运算在以构造误差$L=||(x-\bar{x})-\hat{x}||_2$<strong>最小化</strong>为目标时，是<strong>线性单隐层</strong>神经网络的<strong>最优解</strong>。(Autoencoder)</p>
<blockquote>
<p>PCA的最优解对应<strong>最大特征值</strong>，因此可以使用<strong>梯度下降</strong>求解。（但神经网络中通常无法求得最优解）</p>
</blockquote>
</li>
</ul>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><ul>
<li>也是一种降维压缩的办法，使用不同秩的奇异值矩阵保留不同密度的信息。（秩越大保存的信息越多）</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-74f7d2430ace69c9cd6b963eb58a5079_1440w.jpg" alt="img" style="zoom: 25%;" /></p>
<h3 id="Word-Enbedding"><a href="#Word-Enbedding" class="headerlink" title="Word Enbedding"></a>Word Enbedding</h3><ul>
<li><p>词向量</p>
<ul>
<li><p>基于统计：阅读大量文章，经常一起出现的词所对应的词向量应该有<strong>较高相似度</strong>。</p>
</li>
<li><p>基于预测：使用词的上下文信息建立神经网络，例如前一个单词的<strong>独热编码</strong>作为输入来预测下一个单词。此时神经网络的输出是下一个单词各个类别的概率，通过训练使某个词对应的概率最大化，然后将改神经网络的第一个隐层的<strong>输入</strong>作为词向量。</p>
<blockquote>
<p>因为相似的词需要得到相似的输出，所以第一个隐层的输入作为词向量可以保证它们的相似度。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>涉及到共享权值时，梯度下降更新时需要<strong>减去所有</strong>共享的权值的偏微分与学习率的乘积。</p>
<blockquote>
<ul>
<li>卷积反向传播更新参数<strong>同理</strong></li>
<li>不可导/多微分反向传播：<a href="https://blog.csdn.net/qq_21190081/article/details/72871704" target="_blank" rel="noopener">https://blog.csdn.net/qq_21190081/article/details/72871704</a></li>
</ul>
</blockquote>
</li>
<li><p>共享权值的作用</p>
<ul>
<li><strong>上下文相同</strong>的单词具有<strong>同样的编码</strong>（即相同的权重）</li>
<li>减小模型参数量</li>
</ul>
</li>
</ul>
<h2 id="Explainable-Machine-Learning"><a href="#Explainable-Machine-Learning" class="headerlink" title="Explainable Machine Learning"></a>Explainable Machine Learning</h2><ul>
<li>Local Explanation：找出数据中对预测起到关键作用的部分<ul>
<li>设置一个自定义大小、颜色的蒙版，根据不同的摆放位置判断某区域是否关键。</li>
<li>随机扰动某个pixel，根据结果改变的梯度判断其重要性高低，最终得到salience map.</li>
</ul>
</li>
</ul>
<h2 id="Attack-ML-Model"><a href="#Attack-ML-Model" class="headerlink" title="Attack ML Model"></a>Attack ML Model</h2><ul>
<li><p>损失函数&amp;目标函数&amp;性能表现</p>
<ul>
<li>Loss Function：目标函数在有约束的条件下<strong>需要最小化</strong>的函数</li>
<li>Object Function：需要被优化的函数（最大/最小化均可）</li>
<li>Accuracy/IoU/F1 etc.：供比较的模型的性能指标</li>
</ul>
<blockquote>
<p><strong>Q：为什么不用性能表现作为目标函数？</strong></p>
<p>A：性能表现是<strong>以数据项为单位</strong>的，粒度过粗。例如100张图片分类任务，模型多次迭代后正确率均在90%左右，实际上这90个正确标签<strong>不一定</strong>是相同的，但是准确率不变，<strong>无法通过</strong>梯度下降对模型进行优化。</p>
</blockquote>
</li>
<li><p>攻击时模型的参数固定，改变的是<strong>输入的样本</strong>。</p>
<ul>
<li>通常由梯度下降进行优化<ul>
<li>损失函数：$L\left(x^{\prime}\right)=-C\left(y^{\prime}, y^{\text {true }}\right)+C\left(y^{\prime}, y^{\text {false }}\right)$<ul>
<li>$x’$：输入样本</li>
<li>$y’$：输出结果</li>
<li>$C$：交叉熵损失函数</li>
</ul>
</li>
<li>目标函数：$x^{*}=\arg \min _{d\left(x^{0}, x^{\prime}\right) \leq \varepsilon} L\left(x^{\prime}\right)$</li>
</ul>
</li>
</ul>
</li>
<li>白盒攻击：已知模型结构和参数<ul>
<li>FGSM(Fast Gradient Sign Method)<ul>
<li>$x^{*} \leftarrow x^{0}-\varepsilon \Delta x$</li>
<li>$\Delta x=\left[\begin{array}{c}\operatorname{sign}\left(\partial L / \partial x_{1}\right) \\ \operatorname{sign}\left(\partial L / \partial x_{2}\right) \\ \operatorname{sign}\left(\partial L / \partial x_{3}\right) \\ \vdots\end{array}\right]$only have $+1$ or $-1$</li>
</ul>
</li>
</ul>
</li>
<li>黑盒攻击：训练proxy model进行攻击（白盒攻击），然后使用得到的样本攻击黑盒模型。</li>
<li>被动防御：不修改原始模型，通常在输入模型前增加filter(eg. smoothing/padding/reshape)</li>
<li>主动防御：类似数据增强，在训练时对模型进行攻击，把有害样本加入训练集对模型进行再次训练。</li>
</ul>
<h2 id="Network-Compression"><a href="#Network-Compression" class="headerlink" title="Network Compression"></a>Network Compression</h2><ul>
<li><p>网络剪枝：在给定数据集上将模型计算过程中多次为0的权值和神经元进行删除，然后将小网络再次训练。（以上过程可迭代）</p>
<ul>
<li>实践中，“删除”操作通过将其置为恒0实现。</li>
</ul>
<blockquote>
<p><strong>Q：为什么不直接训练较小的网络？</strong></p>
<p>A：</p>
<ul>
<li>剪枝后的网络结构通常不规则，很难直接构造。</li>
<li>工程经验表明：小网络很难通过优化取得和大网络剪枝相同的效果。</li>
</ul>
</blockquote>
</li>
<li><p>知识蒸馏：用小网络去学习大网络的<strong>完整输出</strong>（eg. 各个label的概率分布），和直接用数据训练小网络不同。</p>
<ul>
<li>使用<strong>集成学习模型</strong>作为大网络可以有效提高小网络的性能</li>
<li>在输入softmax层前将概率值除以$T(Temperature)$，避免大网络的输出变为one-hot，即需要和正确标签保持一定的差异。</li>
</ul>
</li>
<li><p>参数量化</p>
<ul>
<li>改变数据类型，eg. float32 to float16.</li>
<li>权值聚类，最终所有的同类权值均取平均值。（类似colormap）<ul>
<li>可以使用Huffman Coding进一步优化</li>
</ul>
</li>
</ul>
</li>
<li><p>调整网络结构（最有效）</p>
<ul>
<li><p>在隐层间插入含$K$个神经元的全连接层，通过控制$K$来减少参数量。</p>
<p><img src="https://i.loli.net/2021/05/18/cq1yjMaveILkTzd.png" alt="image-20210518105034732" style="zoom:50%;" /></p>
</li>
<li><p>卷积分解：depthwise+pointerwise</p>
<p><img src="https://i.loli.net/2021/05/18/WXJ4PCbVIctwEsS.png" alt="image-20210518105117502" style="zoom:50%;" /></p>
</li>
<li><p>动态计算</p>
<ul>
<li><p>根据环境限制从存储的多个model中选择（内存开销大）</p>
</li>
<li><p>在中间层增加分类器直接进行预测（也可以是其他操作）</p>
<blockquote>
<p>要求<strong>浅层</strong>网络也要提取<strong>深层</strong>信息，可能对<strong>模型整体性能</strong>造成影响。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/23/OS-Course-Note/" rel="prev" title="OS Course Note">
      <i class="fa fa-chevron-left"></i> OS Course Note
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/06/23/Airbnb-Data-Analysis/" rel="next" title="Airbnb Data Analysis">
      Airbnb Data Analysis <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Silence</span>
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  Visitors: <span id="busuanzi_value_site_pv"></span>
</span>
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">|   total: 100.5k words</span>
</div>
  <div class="powered-by">
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>


  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    new Valine(Object.assign({
      el  : '#valine-comments',
      path: location.pathname,
    }, {"enable":true,"appId":"GMneVEFPu4TwpvejAPqllWGc-gzGzoHsz","appKey":"5eNvc7SV1dxRT000WSXjRDuD","placeholder":"Just write what you'd like to write.","avatar":"mm","meta":["nick","mail"],"pageSize":10,"language":"en","visitor":false,"comment_count":true,"recordIP":false,"serverURLs":null}
    ));
  }, window.Valine);
});
</script>

</body>
</html>
